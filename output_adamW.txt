2025-04-12 13:18:39.723888: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-04-12 13:18:39.861547: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1744460319.919109  294826 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1744460319.935298  294826 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1744460320.050895  294826 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744460320.050987  294826 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744460320.050991  294826 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744460320.050993  294826 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-04-12 13:18:40.068451: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[93mWARNING [0m:   DEPRECATED FEATURE: flwr.simulation.start_simulation() is deprecated.
	Instead, use the `flwr run` CLI command to start a local simulation in your Flower app, as shown for example below:

		$ flwr new  # Create a new Flower app from a template

		$ flwr run  # Run the Flower app in Simulation Mode

	Using `start_simulation()` is deprecated.

            This is a deprecated feature. It will be removed
            entirely in future versions of Flower.
        
[92mINFO [0m:      Starting Flower simulation, config: num_rounds=20, no round_timeout
2025-04-12 13:18:46,731	INFO worker.py:1771 -- Started a local Ray instance.
[92mINFO [0m:      Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'CPU': 12.0, 'memory': 3577457051.0, 'object_store_memory': 1788728524.0, 'GPU': 1.0, 'node:192.168.52.78': 1.0}
[92mINFO [0m:      Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html
[92mINFO [0m:      Flower VCE: Resources for each Virtual Client: {'num_cpus': 6}
[92mINFO [0m:      Flower VCE: Creating VirtualClientEngineActorPool with 2 actors
[92mINFO [0m:      [INIT]
[92mINFO [0m:      Requesting initial parameters from one random client
[36m(pid=295712)[0m 2025-04-12 13:18:47.784337: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[36m(pid=295712)[0m 2025-04-12 13:18:47.794643: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
[36m(pid=295712)[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
[36m(pid=295712)[0m E0000 00:00:1744460327.807191  295712 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
[36m(pid=295713)[0m E0000 00:00:1744460327.848564  295713 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
[36m(pid=295713)[0m W0000 00:00:1744460327.858121  295713 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[36m(pid=295713)[0m W0000 00:00:1744460327.858151  295713 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[36m(pid=295713)[0m W0000 00:00:1744460327.858153  295713 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[36m(pid=295713)[0m W0000 00:00:1744460327.858155  295713 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[36m(pid=295713)[0m 2025-04-12 13:18:47.862850: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[36m(pid=295713)[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[36m(ClientAppActor pid=295712)[0m I0000 00:00:1744460329.825659  295712 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9
[92mINFO [0m:      Received initial parameters from one random client
[92mINFO [0m:      Starting evaluation of initial global parameters
[92mINFO [0m:      Evaluation returned no results (`None`)
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 1]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[36m(ClientAppActor pid=295712)[0m I0000 00:00:1744460332.969032  295934 service.cc:152] XLA service 0x7f8b2c0020c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
[36m(ClientAppActor pid=295712)[0m I0000 00:00:1744460332.969103  295934 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9
[36m(ClientAppActor pid=295712)[0m 2025-04-12 13:18:53.029191: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
[36m(pid=295713)[0m 2025-04-12 13:18:47.823019: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[36m(pid=295713)[0m 2025-04-12 13:18:47.833225: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
[36m(ClientAppActor pid=295712)[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(pid=295713)[0m E0000 00:00:1744460327.845091  295713 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
[36m(pid=295712)[0m E0000 00:00:1744460327.810823  295712 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
[36m(pid=295712)[0m W0000 00:00:1744460327.820372  295712 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.[32m [repeated 4x across cluster][0m
[36m(pid=295712)[0m 2025-04-12 13:18:47.823273: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[36m(pid=295712)[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[36m(ClientAppActor pid=295712)[0m I0000 00:00:1744460333.382656  295934 cuda_dnn.cc:529] Loaded cuDNN version 90300
[36m(ClientAppActor pid=295712)[0m 2025-04-12 13:18:54.529155: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10', 12 bytes spill stores, 12 bytes spill loads
[36m(ClientAppActor pid=295712)[0m 
[36m(ClientAppActor pid=295713)[0m 
[36m(ClientAppActor pid=295712)[0m I0000 00:00:1744460336.550391  295934 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[36m(ClientAppActor pid=295713)[0m I0000 00:00:1744460330.537033  295713 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9
[36m(ClientAppActor pid=295713)[0m 
[36m(ClientAppActor pid=295713)[0m I0000 00:00:1744460333.545602  296030 service.cc:152] XLA service 0x2346da90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
[36m(ClientAppActor pid=295713)[0m I0000 00:00:1744460333.545635  296030 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9
[36m(ClientAppActor pid=295713)[0m 2025-04-12 13:18:53.609186: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
[36m(ClientAppActor pid=295713)[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
[36m(ClientAppActor pid=295713)[0m I0000 00:00:1744460334.062248  296030 cuda_dnn.cc:529] Loaded cuDNN version 90300
[36m(ClientAppActor pid=295713)[0m 2025-04-12 13:19:18.500742: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10', 12 bytes spill stores, 12 bytes spill loads[32m [repeated 2x across cluster][0m
[36m(ClientAppActor pid=295713)[0m I0000 00:00:1744460337.049414  296030 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[93mWARNING [0m:   No fit_metrics_aggregation_fn provided
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[36m(ClientAppActor pid=295712)[0m 
[36m(ClientAppActor pid=295712)[0m 2025-04-12 13:19:41.658426: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4', 12 bytes spill stores, 12 bytes spill loads
[36m(ClientAppActor pid=295713)[0m 2025-04-12 13:19:45.325199: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4', 12 bytes spill stores, 12 bytes spill loads
[36m(ClientAppActor pid=295713)[0m 
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[93mWARNING [0m:   No evaluate_metrics_aggregation_fn provided
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 2]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[36m(ClientAppActor pid=295712)[0m 
[36m(ClientAppActor pid=295712)[0m 2025-04-12 13:19:56.106672: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10', 12 bytes spill stores, 12 bytes spill loads
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 3]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 4]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[36m(ClientAppActor pid=295712)[0m 2025-04-12 13:20:22.707197: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4', 12 bytes spill stores, 12 bytes spill loads
[36m(ClientAppActor pid=295712)[0m 
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 5]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[36m(ClientAppActor pid=295712)[0m 2025-04-12 13:20:29.562388: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10', 12 bytes spill stores, 12 bytes spill loads
[36m(ClientAppActor pid=295712)[0m 
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 6]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 7]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[33m(raylet)[0m [2025-04-12 13:20:46,661 E 295011 295011] (raylet) node_manager.cc:3064: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2, IP: 192.168.52.78) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.52.78`
[33m(raylet)[0m 
[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 8]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 9]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 10]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 11]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[36m(ClientAppActor pid=295712)[0m 2025-04-12 13:21:29.627524: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_10', 12 bytes spill stores, 12 bytes spill loads
[36m(ClientAppActor pid=295712)[0m 
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 12]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 13]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 14]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 15]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 10 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 10 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 16]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 10 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 10 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 17]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 10 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 10 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 18]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 10 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 10 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 19]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 10 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 10 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 20]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 0 results and 10 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: 554475c17c9bde98312085c501000000, name=ClientAppActor.__init__, pid=295713, memory used=1.86GB) was running was 22.27GB / 22.49GB (0.990203), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-00e4c93a99f584e77389d2ce6ee30f8b4b33bf558eb3c0a1afba9a36*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
295712	1.89	ray::ClientAppActor.run
295713	1.86	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
215737	0.46	python3 fl_model/simulate_flower.py
294826	0.44	python3 fl_model/simulate_flower.py
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 0db90a5c99020e1d7bc853d0783a666a83f5785250a0b9cd38e968c2) where the task (actor ID: f58bf37c2097888fb65cee9801000000, name=ClientAppActor.__init__, pid=295712, memory used=3.81GB) was running was 22.27GB / 22.49GB (0.990481), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-922f4283139de58bc2f6b7a5323c0543b8c1cc5e5bb9508e3c48307f*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
216430	4.87	ray::ClientAppActor.run
295712	3.81	ray::ClientAppActor.run
162997	2.52	ray::ClientAppActor.run
162996	2.48	ray::ClientAppActor.run
112417	0.68	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node /home/timotei/.vscode...
212450	0.62	ray::ClientAppActor.run
212449	0.61	ray::ClientAppActor.run
294826	0.49	python3 fl_model/simulate_flower.py
215737	0.46	python3 fl_model/simulate_flower.py
112216	0.45	/home/timotei/.vscode-server/bin/4949701c880d4bdb949e3c0e6b400288da7f474b/node --dns-result-order=ip...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 0 results and 10 failures
[92mINFO [0m:      
[92mINFO [0m:      [SUMMARY]
[92mINFO [0m:      Run finished 20 round(s) in 200.37s
[92mINFO [0m:      	History (loss, distributed):
[92mINFO [0m:      		round 1: 309.8247665148823
[92mINFO [0m:      		round 2: 63.242671966552734
[92mINFO [0m:      		round 3: 53.160770416259766
[92mINFO [0m:      		round 4: 38.83647537231445
[92mINFO [0m:      		round 5: 66.9303970336914
[92mINFO [0m:      		round 6: 32.858150482177734
[92mINFO [0m:      		round 7: 63.741214752197266
[92mINFO [0m:      		round 8: 37.116153717041016
[92mINFO [0m:      		round 9: 53.792232513427734
[92mINFO [0m:      		round 10: 72.2258529663086
[92mINFO [0m:      		round 11: 49.97636795043945
[92mINFO [0m:      		round 12: 95.40979766845703
[92mINFO [0m:      		round 13: 40.86264419555664
[92mINFO [0m:      		round 14: 63.791927337646484
[92mINFO [0m:      
Memory growth enabled on GPUs
[36m(ClientAppActor pid=295712)[0m Memory growth enabled on GPUs
[36m(ClientAppActor pid=295713)[0m Full evaluation results: [317.1689453125, 217.9431610107422, 718.9427490234375, 0.005814232397824526, 8.88253116607666, 26.848325729370117, 0.05618561804294586]
[36m(ClientAppActor pid=295713)[0m [CLIENT] Duration MAE = 8.8825, Lat MAE = 26.8483, Lon MAE = 0.0562
[36m(ClientAppActor pid=295713)[0m Memory growth enabled on GPUs
[36m(ClientAppActor pid=295712)[0m Full evaluation results: [294.1408996582031, 171.85568237304688, 737.4230346679688, 0.0069341170601546764, 7.849639415740967, 27.08133316040039, 0.06411271542310715][32m [repeated 6x across cluster][0m
[36m(ClientAppActor pid=295712)[0m [CLIENT] Duration MAE = 7.8496, Lat MAE = 27.0813, Lon MAE = 0.0641[32m [repeated 6x across cluster][0m
[36m(ClientAppActor pid=295712)[0m Full evaluation results: [63.242671966552734, 150.0718536376953, 28.72095489501953, 0.001819392666220665, 9.3104829788208, 4.402514457702637, 0.03528670594096184][32m [repeated 4x across cluster][0m
[36m(ClientAppActor pid=295712)[0m [CLIENT] Duration MAE = 9.3105, Lat MAE = 4.4025, Lon MAE = 0.0353[32m [repeated 4x across cluster][0m
[36m(ClientAppActor pid=295712)[0m Full evaluation results: [53.160770416259766, 152.2591552734375, 6.287301540374756, 0.004334259312599897, 8.742116928100586, 2.2840394973754883, 0.05390762537717819]
[36m(ClientAppActor pid=295712)[0m [CLIENT] Duration MAE = 8.7421, Lat MAE = 2.2840, Lon MAE = 0.0539
[36m(ClientAppActor pid=295712)[0m Full evaluation results: [38.83647537231445, 107.2431640625, 8.97014045715332, 0.0010306410258635879, 7.5848069190979, 2.5577919483184814, 0.02582682855427265]
[36m(ClientAppActor pid=295712)[0m [CLIENT] Duration MAE = 7.5848, Lat MAE = 2.5578, Lon MAE = 0.0258
[36m(ClientAppActor pid=295712)[0m Full evaluation results: [66.9303970336914, 178.94351196289062, 6.918929576873779, 0.002822620328515768, 8.426543235778809, 2.3748064041137695, 0.04641731455922127]
[36m(ClientAppActor pid=295712)[0m [CLIENT] Duration MAE = 8.4265, Lat MAE = 2.3748, Lon MAE = 0.0464
[36m(ClientAppActor pid=295712)[0m Full evaluation results: [32.858150482177734, 90.93106842041016, 0.6534719467163086, 0.0012881419388577342, 7.234190940856934, 0.6259154677391052, 0.029651319608092308]
[36m(ClientAppActor pid=295712)[0m [CLIENT] Duration MAE = 7.2342, Lat MAE = 0.6259, Lon MAE = 0.0297
[36m(ClientAppActor pid=295712)[0m Full evaluation results: [63.741214752197266, 177.6704864501953, 1.4547356367111206, 0.0017920664977282286, 9.262097358703613, 1.157489538192749, 0.03468025475740433]
[36m(ClientAppActor pid=295712)[0m [CLIENT] Duration MAE = 9.2621, Lat MAE = 1.1575, Lon MAE = 0.0347
[36m(ClientAppActor pid=295712)[0m Full evaluation results: [37.116153717041016, 108.78450012207031, 2.639190435409546, 0.002267348812893033, 7.893705368041992, 1.3446253538131714, 0.04161600023508072]
[36m(ClientAppActor pid=295712)[0m [CLIENT] Duration MAE = 7.8937, Lat MAE = 1.3446, Lon MAE = 0.0416
[36m(ClientAppActor pid=295712)[0m Full evaluation results: [53.792232513427734, 146.2287139892578, 0.5600688457489014, 0.0012756322976201773, 8.863763809204102, 0.5691211223602295, 0.030609525740146637]
[36m(ClientAppActor pid=295712)[0m [CLIENT] Duration MAE = 8.8638, Lat MAE = 0.5691, Lon MAE = 0.0306
[36m(ClientAppActor pid=295712)[0m Full evaluation results: [72.2258529663086, 217.08413696289062, 1.1620275974273682, 0.0024294499307870865, 10.83973217010498, 0.8699906468391418, 0.041731398552656174]
[36m(ClientAppActor pid=295712)[0m [CLIENT] Duration MAE = 10.8397, Lat MAE = 0.8700, Lon MAE = 0.0417
[36m(ClientAppActor pid=295712)[0m Full evaluation results: [49.97636795043945, 155.68931579589844, 2.330399990081787, 0.0013400190509855747, 9.52920150756836, 1.0765148401260376, 0.02846078760921955]
[36m(ClientAppActor pid=295712)[0m [CLIENT] Duration MAE = 9.5292, Lat MAE = 1.0765, Lon MAE = 0.0285
[36m(ClientAppActor pid=295712)[0m Full evaluation results: [95.40979766845703, 271.1862487792969, 2.285142660140991, 0.00287284841760993, 10.707696914672852, 1.0711437463760376, 0.04606383666396141]
[36m(ClientAppActor pid=295712)[0m [CLIENT] Duration MAE = 10.7077, Lat MAE = 1.0711, Lon MAE = 0.0461
[36m(ClientAppActor pid=295712)[0m Full evaluation results: [40.86264419555664, 135.42579650878906, 0.2560074031352997, 0.0012083705514669418, 7.7071967124938965, 0.3720579743385315, 0.02777169831097126]
[36m(ClientAppActor pid=295712)[0m [CLIENT] Duration MAE = 7.7072, Lat MAE = 0.3721, Lon MAE = 0.0278
[36m(ClientAppActor pid=295712)[0m Full evaluation results: [63.791927337646484, 177.72328186035156, 0.6778007745742798, 0.0014472116017714143, 8.751564025878906, 0.6636128425598145, 0.03159400075674057]
[36m(ClientAppActor pid=295712)[0m [CLIENT] Duration MAE = 8.7516, Lat MAE = 0.6636, Lon MAE = 0.0316

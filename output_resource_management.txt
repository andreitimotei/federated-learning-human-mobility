2025-04-07 18:38:13.246372: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-04-07 18:38:13.385571: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1744047493.439637   44889 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1744047493.455122   44889 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1744047493.570571   44889 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744047493.570634   44889 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744047493.570637   44889 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744047493.570639   44889 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-04-07 18:38:13.586701: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[93mWARNING [0m:   DEPRECATED FEATURE: flwr.simulation.start_simulation() is deprecated.
	Instead, use the `flwr run` CLI command to start a local simulation in your Flower app, as shown for example below:

		$ flwr new  # Create a new Flower app from a template

		$ flwr run  # Run the Flower app in Simulation Mode

	Using `start_simulation()` is deprecated.

            This is a deprecated feature. It will be removed
            entirely in future versions of Flower.
        
[92mINFO [0m:      Starting Flower simulation, config: num_rounds=20, no round_timeout
2025-04-07 18:38:20,426	INFO worker.py:1771 -- Started a local Ray instance.
[92mINFO [0m:      Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'CPU': 4.0, 'memory': 11626133915.0, 'object_store_memory': 5813066956.0, 'GPU': 1.0, 'node:192.168.52.78': 1.0}
[92mINFO [0m:      Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html
[92mINFO [0m:      Flower VCE: Resources for each Virtual Client: {'num_cpus': 1}
[92mINFO [0m:      Flower VCE: Creating VirtualClientEngineActorPool with 4 actors
[92mINFO [0m:      [INIT]
[92mINFO [0m:      Requesting initial parameters from one random client
[36m(pid=45270)[0m 2025-04-07 18:38:21.330514: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[36m(pid=45270)[0m 2025-04-07 18:38:21.340938: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
[36m(pid=45270)[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
[36m(pid=45270)[0m E0000 00:00:1744047501.354790   45270 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
[36m(pid=45270)[0m E0000 00:00:1744047501.358938   45270 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
[36m(pid=45270)[0m W0000 00:00:1744047501.375319   45270 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[36m(pid=45270)[0m W0000 00:00:1744047501.375339   45270 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[36m(pid=45270)[0m W0000 00:00:1744047501.375340   45270 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[36m(pid=45270)[0m W0000 00:00:1744047501.375341   45270 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[36m(pid=45270)[0m 2025-04-07 18:38:21.378485: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[36m(pid=45270)[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[36m(ClientAppActor pid=45270)[0m I0000 00:00:1744047503.480453   45270 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9
[92mINFO [0m:      Received initial parameters from one random client
[92mINFO [0m:      Starting evaluation of initial global parameters
[92mINFO [0m:      Evaluation returned no results (`None`)
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 1]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[36m(ClientAppActor pid=45270)[0m I0000 00:00:1744047505.946016   45445 service.cc:152] XLA service 0x7f9ea8015de0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
[36m(ClientAppActor pid=45270)[0m I0000 00:00:1744047505.946105   45445 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9
[36m(ClientAppActor pid=45270)[0m 2025-04-07 18:38:26.011362: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
[36m(ClientAppActor pid=45270)[0m I0000 00:00:1744047509.247937   45445 cuda_dnn.cc:529] Loaded cuDNN version 90300
[36m(pid=45267)[0m 2025-04-07 18:38:21.404336: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(pid=45267)[0m 2025-04-07 18:38:21.417092: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered[32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=45270)[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR[32m [repeated 4x across cluster][0m
[36m(pid=45267)[0m E0000 00:00:1744047501.431418   45267 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered[32m [repeated 3x across cluster][0m
[36m(pid=45267)[0m E0000 00:00:1744047501.436026   45267 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered[32m [repeated 3x across cluster][0m
[36m(pid=45267)[0m W0000 00:00:1744047501.445710   45267 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.[32m [repeated 12x across cluster][0m
[36m(pid=45267)[0m 2025-04-07 18:38:21.449536: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 3x across cluster][0m
[36m(pid=45267)[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=45267)[0m I0000 00:00:1744047504.262426   45267 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9[32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=45270)[0m 2025-04-07 18:38:30.167670: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1564', 52 bytes spill stores, 52 bytes spill loads
[36m(ClientAppActor pid=45270)[0m 
[36m(ClientAppActor pid=45270)[0m 2025-04-07 18:38:30.398308: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1564', 440 bytes spill stores, 440 bytes spill loads
[36m(ClientAppActor pid=45270)[0m 
[36m(ClientAppActor pid=45267)[0m 
[36m(ClientAppActor pid=45269)[0m 
[36m(ClientAppActor pid=45268)[0m I0000 00:00:1744047509.609221   45556 service.cc:152] XLA service 0x7f10cc015e60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:[32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=45268)[0m I0000 00:00:1744047509.609304   45556 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9[32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=45268)[0m 2025-04-07 18:38:29.663555: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.[32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=45269)[0m 
[36m(ClientAppActor pid=45268)[0m 
[36m(ClientAppActor pid=45268)[0m 
[36m(ClientAppActor pid=45267)[0m 
[36m(ClientAppActor pid=45270)[0m I0000 00:00:1744047513.074126   45445 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[36m(ClientAppActor pid=45270)[0m 
[36m(ClientAppActor pid=45267)[0m I0000 00:00:1744047510.029024   45562 cuda_dnn.cc:529] Loaded cuDNN version 90300[32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=45268)[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR[32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=45270)[0m 
[36m(ClientAppActor pid=45268)[0m 
[36m(ClientAppActor pid=45268)[0m 2025-04-07 18:38:35.251043: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1497', 52 bytes spill stores, 52 bytes spill loads[32m [repeated 9x across cluster][0m
[36m(ClientAppActor pid=45268)[0m 
[36m(ClientAppActor pid=45267)[0m 
[36m(ClientAppActor pid=45269)[0m 
[36m(ClientAppActor pid=45267)[0m 
[36m(ClientAppActor pid=45269)[0m 
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[93mWARNING [0m:   No fit_metrics_aggregation_fn provided
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[93mWARNING [0m:   No evaluate_metrics_aggregation_fn provided
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 2]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 3]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 4]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 5]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 6]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 7]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 8]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 9]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 10]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 11]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 12]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 13]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 14]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 15]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 16]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 9 results and 1 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 3 results and 7 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 17]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 3 results and 7 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 3 results and 7 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 18]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 3 results and 7 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 3 results and 7 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 19]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 3 results and 7 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 3 results and 7 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 20]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 3 results and 7 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 913c21855d571a9ca3e3aa5ae27d55b7598fd61320f91ba132f3572f) where the task (actor ID: 3da588760ddf7bd7d8471d2b01000000, name=ClientAppActor.__init__, pid=45267, memory used=3.82GB) was running was 19.35GB / 19.54GB (0.990351), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-90241a25bdc4cb060a823d9d8d2252a6288dd9de7039afcd6ae7c798*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
45270	4.55	ray::ClientAppActor.run
45268	4.52	ray::ClientAppActor.run
45269	3.99	ray::ClientAppActor.run
45267	3.82	ray::ClientAppActor.run
9927	0.30	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
44928	0.13	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
44889	0.13	python3 fl_model/simulate_flower.py
10454	0.12	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
45012	0.10	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
44975	0.06	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 3 results and 7 failures
[92mINFO [0m:      
[92mINFO [0m:      [SUMMARY]
[92mINFO [0m:      Run finished 20 round(s) in 373.99s
[92mINFO [0m:      	History (loss, distributed):
[92mINFO [0m:      		round 1: 325.7057189941406
[92mINFO [0m:      		round 2: 332.4381408691406
[92mINFO [0m:      		round 3: 330.6839294433594
[92mINFO [0m:      		round 4: 323.0739440917969
[92mINFO [0m:      		round 5: 320.69757080078125
[92mINFO [0m:      		round 6: 316.32037353515625
[92mINFO [0m:      		round 7: 315.5624694824219
[92mINFO [0m:      		round 8: 306.0308532714844
[92mINFO [0m:      		round 9: 304.0054931640625
[92mINFO [0m:      		round 10: 292.6780090332031
[92mINFO [0m:      		round 11: 284.12567138671875
[92mINFO [0m:      		round 12: 265.5894470214844
[92mINFO [0m:      		round 13: 250.5034637451172
[92mINFO [0m:      		round 14: 246.25331115722656
[92mINFO [0m:      		round 15: 235.396728515625
[92mINFO [0m:      		round 16: 228.6370391845703
[92mINFO [0m:      		round 17: 219.18603515625
[92mINFO [0m:      		round 18: 216.087158203125
[92mINFO [0m:      		round 19: 215.0113983154297
[92mINFO [0m:      		round 20: 205.97532653808594
[92mINFO [0m:      
Memory growth enabled on GPUs
[36m(ClientAppActor pid=45270)[0m Memory growth enabled on GPUs
[36m(ClientAppActor pid=45269)[0m Full evaluation results: [325.7057189941406, 528.5030517578125, 6.689055919647217, 0.006976744160056114, 15.072936058044434]
[36m(ClientAppActor pid=45269)[0m [CLIENT] Duration MAE = 0.0070, Destination Accuracy = 15.0729
[36m(ClientAppActor pid=45267)[0m Memory growth enabled on GPUs[32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=45269)[0m Full evaluation results: [332.4381408691406, 539.3030395507812, 6.56687593460083, 0.0023255813866853714, 15.422770500183105][32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=45269)[0m [CLIENT] Duration MAE = 0.0023, Destination Accuracy = 15.4228[32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=45269)[0m Full evaluation results: [330.6839294433594, 535.9494018554688, 6.455296993255615, 0.013953488320112228, 15.298583030700684][32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=45269)[0m [CLIENT] Duration MAE = 0.0140, Destination Accuracy = 15.2986[32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=45269)[0m Full evaluation results: [323.0739440917969, 523.1171875, 6.333115100860596, 0.013953488320112228, 14.86488151550293][32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=45269)[0m [CLIENT] Duration MAE = 0.0140, Destination Accuracy = 14.8649[32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=45269)[0m Full evaluation results: [320.69757080078125, 519.5070190429688, 6.264374732971191, 0.030232558026909828, 14.734962463378906][32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=45269)[0m [CLIENT] Duration MAE = 0.0302, Destination Accuracy = 14.7350[32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=45270)[0m Full evaluation results: [316.32037353515625, 512.4022216796875, 6.161332607269287, 0.023255813866853714, 14.485601425170898][32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=45270)[0m [CLIENT] Duration MAE = 0.0233, Destination Accuracy = 14.4856[32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=45269)[0m Full evaluation results: [315.5624694824219, 511.19281005859375, 5.965657711029053, 0.0162790697067976, 14.43717098236084][32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=45269)[0m [CLIENT] Duration MAE = 0.0163, Destination Accuracy = 14.4372[32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=45269)[0m Full evaluation results: [306.0308532714844, 495.26751708984375, 5.881854057312012, 0.013953488320112228, 13.8716459274292][32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=45269)[0m [CLIENT] Duration MAE = 0.0140, Destination Accuracy = 13.8716[32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=45269)[0m Full evaluation results: [304.0054931640625, 491.93560791015625, 5.809947967529297, 0.0162790697067976, 13.778230667114258][32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=45269)[0m [CLIENT] Duration MAE = 0.0163, Destination Accuracy = 13.7782[32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=45269)[0m Full evaluation results: [292.6780090332031, 473.2674255371094, 5.66635274887085, 0.023255813866853714, 13.089295387268066][32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=45269)[0m [CLIENT] Duration MAE = 0.0233, Destination Accuracy = 13.0893[32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=45269)[0m Full evaluation results: [284.12567138671875, 459.20977783203125, 5.593655109405518, 0.020930232480168343, 12.580656051635742][32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=45269)[0m [CLIENT] Duration MAE = 0.0209, Destination Accuracy = 12.5807[32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=45270)[0m Full evaluation results: [265.5894470214844, 428.36846923828125, 5.547167778015137, 0.027906976640224457, 11.472771644592285][32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=45270)[0m [CLIENT] Duration MAE = 0.0279, Destination Accuracy = 11.4728[32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=45269)[0m Full evaluation results: [250.5034637451172, 403.46392822265625, 5.5883469581604, 0.025581395253539085, 10.624382019042969][32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=45269)[0m [CLIENT] Duration MAE = 0.0256, Destination Accuracy = 10.6244[32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=45267)[0m Full evaluation results: [246.25331115722656, 396.4114685058594, 5.587416648864746, 0.0325581394135952, 10.370088577270508][32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=45267)[0m [CLIENT] Duration MAE = 0.0326, Destination Accuracy = 10.3701[32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=45269)[0m Full evaluation results: [235.396728515625, 378.4751281738281, 5.635797023773193, 0.03720930218696594, 9.84048080444336][32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=45269)[0m [CLIENT] Duration MAE = 0.0372, Destination Accuracy = 9.8405[32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=45268)[0m Full evaluation results: [228.6370391845703, 367.0545349121094, 5.71704626083374, 0.04883721098303795, 9.4999361038208][32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=45268)[0m [CLIENT] Duration MAE = 0.0488, Destination Accuracy = 9.4999[32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=45268)[0m Full evaluation results: [219.18603515625, 351.61041259765625, 5.778461456298828, 0.03488372266292572, 9.098987579345703][32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=45268)[0m [CLIENT] Duration MAE = 0.0349, Destination Accuracy = 9.0990[32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=45269)[0m Full evaluation results: [216.087158203125, 346.18017578125, 5.907320499420166, 0.04883721098303795, 9.131961822509766][32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=45269)[0m [CLIENT] Duration MAE = 0.0488, Destination Accuracy = 9.1320[32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=45269)[0m Full evaluation results: [215.0113983154297, 344.26104736328125, 5.973427772521973, 0.055813953280448914, 9.175458908081055][32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=45269)[0m [CLIENT] Duration MAE = 0.0558, Destination Accuracy = 9.1755[32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=45269)[0m Full evaluation results: [205.97532653808594, 329.36376953125, 6.005392551422119, 0.055813953280448914, 9.072031021118164][32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=45269)[0m [CLIENT] Duration MAE = 0.0558, Destination Accuracy = 9.0720[32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=45267)[0m I0000 00:00:1744047513.836531   45562 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.[32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=45269)[0m 2025-04-07 18:38:36.447595: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1497', 268 bytes spill stores, 268 bytes spill loads[32m [repeated 5x across cluster][0m
[36m(ClientAppActor pid=45268)[0m Full evaluation results: [205.97532653808594, 329.36376953125, 6.005392551422119, 0.055813953280448914, 9.072031021118164][32m [repeated 2x across cluster][0m
[36m(ClientAppActor pid=45268)[0m [CLIENT] Duration MAE = 0.0558, Destination Accuracy = 9.0720[32m [repeated 2x across cluster][0m

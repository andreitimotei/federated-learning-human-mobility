2025-04-08 14:23:48.510293: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-04-08 14:23:48.642953: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1744118628.696386   64994 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1744118628.711548   64994 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1744118628.821687   64994 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744118628.821752   64994 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744118628.821755   64994 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744118628.821757   64994 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-04-08 14:23:48.838172: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[93mWARNING [0m:   DEPRECATED FEATURE: flwr.simulation.start_simulation() is deprecated.
	Instead, use the `flwr run` CLI command to start a local simulation in your Flower app, as shown for example below:

		$ flwr new  # Create a new Flower app from a template

		$ flwr run  # Run the Flower app in Simulation Mode

	Using `start_simulation()` is deprecated.

            This is a deprecated feature. It will be removed
            entirely in future versions of Flower.
        
[92mINFO [0m:      Starting Flower simulation, config: num_rounds=20, no round_timeout
2025-04-08 14:23:55,696	INFO worker.py:1771 -- Started a local Ray instance.
[92mINFO [0m:      Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'CPU': 4.0, 'object_store_memory': 5490232934.0, 'memory': 10980465870.0, 'GPU': 1.0, 'node:192.168.52.78': 1.0}
[92mINFO [0m:      Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html
[92mINFO [0m:      Flower VCE: Resources for each Virtual Client: {'num_cpus': 1}
[92mINFO [0m:      Flower VCE: Creating VirtualClientEngineActorPool with 4 actors
[92mINFO [0m:      [INIT]
[92mINFO [0m:      Requesting initial parameters from one random client
[36m(pid=65521)[0m 2025-04-08 14:23:56.792750: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[36m(pid=65521)[0m 2025-04-08 14:23:56.810739: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
[36m(pid=65521)[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
[36m(pid=65521)[0m E0000 00:00:1744118636.827347   65521 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
[36m(pid=65521)[0m E0000 00:00:1744118636.832377   65521 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
[36m(pid=65521)[0m W0000 00:00:1744118636.847730   65521 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[36m(pid=65521)[0m W0000 00:00:1744118636.847763   65521 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[36m(pid=65521)[0m W0000 00:00:1744118636.847765   65521 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[36m(pid=65521)[0m W0000 00:00:1744118636.847766   65521 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[36m(pid=65521)[0m 2025-04-08 14:23:56.852000: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[36m(pid=65521)[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[36m(ClientAppActor pid=65521)[0m I0000 00:00:1744118639.433347   65521 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9
[92mINFO [0m:      Received initial parameters from one random client
[92mINFO [0m:      Starting evaluation of initial global parameters
[92mINFO [0m:      Evaluation returned no results (`None`)
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 1]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[36m(ClientAppActor pid=65521)[0m I0000 00:00:1744118643.047561   65786 service.cc:152] XLA service 0x7fb7500027d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
[36m(ClientAppActor pid=65521)[0m I0000 00:00:1744118643.047796   65786 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9
[36m(pid=65519)[0m 2025-04-08 14:23:56.824403: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(pid=65519)[0m 2025-04-08 14:23:56.836728: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered[32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=65521)[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR[32m [repeated 4x across cluster][0m
[36m(pid=65519)[0m E0000 00:00:1744118636.853184   65519 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered[32m [repeated 3x across cluster][0m
[36m(pid=65519)[0m E0000 00:00:1744118636.857888   65519 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered[32m [repeated 3x across cluster][0m
[36m(pid=65519)[0m W0000 00:00:1744118636.873575   65519 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.[32m [repeated 12x across cluster][0m
[36m(pid=65519)[0m 2025-04-08 14:23:56.877088: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.[32m [repeated 3x across cluster][0m
[36m(pid=65519)[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.[32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=65521)[0m 2025-04-08 14:24:03.099792: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
[36m(ClientAppActor pid=65521)[0m I0000 00:00:1744118643.435208   65786 cuda_dnn.cc:529] Loaded cuDNN version 90300
[36m(ClientAppActor pid=65521)[0m 2025-04-08 14:24:04.468172: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1564', 52 bytes spill stores, 52 bytes spill loads
[36m(ClientAppActor pid=65521)[0m 
[36m(ClientAppActor pid=65519)[0m I0000 00:00:1744118640.431442   65519 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9[32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=65521)[0m 2025-04-08 14:24:04.588628: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1564', 440 bytes spill stores, 440 bytes spill loads
[36m(ClientAppActor pid=65521)[0m 
[36m(ClientAppActor pid=65518)[0m 
[36m(ClientAppActor pid=65520)[0m 
[36m(ClientAppActor pid=65520)[0m 
[36m(ClientAppActor pid=65518)[0m 
[36m(ClientAppActor pid=65519)[0m 
[36m(ClientAppActor pid=65519)[0m 
[36m(ClientAppActor pid=65521)[0m I0000 00:00:1744118646.810202   65786 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[36m(ClientAppActor pid=65521)[0m 
[36m(ClientAppActor pid=65519)[0m I0000 00:00:1744118643.843001   65962 service.cc:152] XLA service 0x7f4290006f90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:[32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=65519)[0m I0000 00:00:1744118643.843046   65962 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9[32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=65519)[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR[32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=65521)[0m 
[36m(ClientAppActor pid=65519)[0m 2025-04-08 14:24:03.896867: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.[32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=65520)[0m 
[36m(ClientAppActor pid=65519)[0m I0000 00:00:1744118644.351501   65962 cuda_dnn.cc:529] Loaded cuDNN version 90300[32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=65520)[0m 
[36m(ClientAppActor pid=65520)[0m 2025-04-08 14:24:09.422981: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1497', 268 bytes spill stores, 268 bytes spill loads[32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=65518)[0m 
[36m(ClientAppActor pid=65518)[0m 
[36m(ClientAppActor pid=65519)[0m 
[36m(ClientAppActor pid=65519)[0m 
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[93mWARNING [0m:   No fit_metrics_aggregation_fn provided
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[93mWARNING [0m:   No evaluate_metrics_aggregation_fn provided
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 2]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 3]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 4]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 5]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 6]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 7]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 8]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 9]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 10]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 11]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 12]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 13]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 14]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 15]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 6 results and 4 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[33m(raylet)[0m [2025-04-08 14:29:55,621 E 65174 65174] (raylet) node_manager.cc:3064: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1, IP: 192.168.52.78) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.52.78`
[33m(raylet)[0m 
[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[36m(ClientAppActor pid=65519)[0m I0000 00:00:1744118647.911674   65962 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.[32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=65519)[0m 2025-04-08 14:24:10.189938: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1497', 268 bytes spill stores, 268 bytes spill loads[32m [repeated 4x across cluster][0m
[92mINFO [0m:      aggregate_evaluate: received 3 results and 7 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 16]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 3 results and 7 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 3 results and 7 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 17]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 3 results and 7 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 3 results and 7 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 18]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 3 results and 7 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 3 results and 7 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 19]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 3 results and 7 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 3 results and 7 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 20]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 3 results and 7 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 3525da928f162682db6328e64b438671e058ab4dcfbc4ce7e38123d1) where the task (actor ID: 07526101ac618311b3e24fc801000000, name=ClientAppActor.__init__, pid=65519, memory used=3.96GB) was running was 19.36GB / 19.54GB (0.991097), which exceeds the memory usage threshold of 0.99. Ray killed this worker (ID: 6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-6687d9d68e0eafa17ddb7497a861342d2a88f3aa93ecdff3722d6caf*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
65520	4.24	ray::ClientAppActor.run
65521	4.06	ray::ClientAppActor.run
65518	4.05	ray::ClientAppActor.run
65519	3.96	ray::ClientAppActor.run
556	0.98	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
65051	0.20	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
65174	0.16	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
823	0.14	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
64994	0.14	python3 fl_model/simulate_flower.py
65122	0.10	/home/timotei/venv/bin/python3 /home/timotei/venv/lib/python3.12/site-packages/ray/dashboard/dashboa...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 3 results and 7 failures
[92mINFO [0m:      
[92mINFO [0m:      [SUMMARY]
[92mINFO [0m:      Run finished 20 round(s) in 390.99s
[92mINFO [0m:      	History (loss, distributed):
[92mINFO [0m:      		round 1: 334.8099060058594
[92mINFO [0m:      		round 2: 340.2840576171875
[92mINFO [0m:      		round 3: 334.5928039550781
[92mINFO [0m:      		round 4: 331.5690002441406
[92mINFO [0m:      		round 5: 315.5106506347656
[92mINFO [0m:      		round 6: 314.8330078125
[92mINFO [0m:      		round 7: 303.4074401855469
[92mINFO [0m:      		round 8: 302.5248107910156
[92mINFO [0m:      		round 9: 299.2713928222656
[92mINFO [0m:      		round 10: 287.4095458984375
[92mINFO [0m:      		round 11: 278.0940856933594
[92mINFO [0m:      		round 12: 260.4585266113281
[92mINFO [0m:      		round 13: 246.7294921875
[92mINFO [0m:      		round 14: 245.22792053222656
[92mINFO [0m:      		round 15: 237.72247314453125
[92mINFO [0m:      		round 16: 225.51318359375
[92mINFO [0m:      		round 17: 216.93067932128906
[92mINFO [0m:      		round 18: 204.60409545898438
[92mINFO [0m:      		round 19: 203.04837036132812
[92mINFO [0m:      		round 20: 200.0489501953125
[92mINFO [0m:      
Memory growth enabled on GPUs
[36m(ClientAppActor pid=65521)[0m Memory growth enabled on GPUs
[36m(ClientAppActor pid=65520)[0m Full evaluation results: [334.8099060058594, 543.7124633789062, 6.674422740936279, 0.011627906933426857, 15.56407642364502]
[36m(ClientAppActor pid=65520)[0m [CLIENT] Duration MAE = 0.0116, Destination Accuracy = 15.5641
[36m(ClientAppActor pid=65519)[0m Memory growth enabled on GPUs[32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=65521)[0m Full evaluation results: [340.2840576171875, 552.3611450195312, 6.657946586608887, 0.023255813866853714, 15.852108001708984][32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=65521)[0m [CLIENT] Duration MAE = 0.0233, Destination Accuracy = 15.8521[32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=65521)[0m Full evaluation results: [334.5928039550781, 542.4730224609375, 6.577434539794922, 0.0162790697067976, 15.535500526428223][32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=65521)[0m [CLIENT] Duration MAE = 0.0163, Destination Accuracy = 15.5355[32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=65521)[0m Full evaluation results: [331.5690002441406, 537.0718994140625, 6.465646266937256, 0.009302325546741486, 15.341141700744629][32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=65521)[0m [CLIENT] Duration MAE = 0.0093, Destination Accuracy = 15.3411[32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=65521)[0m Full evaluation results: [315.5106506347656, 511.0757141113281, 6.295224189758301, 0.0162790697067976, 14.516594886779785][32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=65521)[0m [CLIENT] Duration MAE = 0.0163, Destination Accuracy = 14.5166[32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=65521)[0m Full evaluation results: [314.8330078125, 509.66796875, 6.159407138824463, 0.027906976640224457, 14.415483474731445][32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=65521)[0m [CLIENT] Duration MAE = 0.0279, Destination Accuracy = 14.4155[32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=65521)[0m Full evaluation results: [303.4074401855469, 491.10369873046875, 6.018525123596191, 0.041860464960336685, 13.800686836242676][32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=65521)[0m [CLIENT] Duration MAE = 0.0419, Destination Accuracy = 13.8007[32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=65521)[0m Full evaluation results: [302.5248107910156, 489.6512756347656, 5.919922351837158, 0.0325581394135952, 13.734484672546387][32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=65521)[0m [CLIENT] Duration MAE = 0.0326, Destination Accuracy = 13.7345[32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=65518)[0m Full evaluation results: [299.2713928222656, 484.0566711425781, 5.8777570724487305, 0.0325581394135952, 13.49322509765625][32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=65518)[0m [CLIENT] Duration MAE = 0.0326, Destination Accuracy = 13.4932[32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=65521)[0m Full evaluation results: [287.4095458984375, 464.82818603515625, 5.698484897613525, 0.0325581394135952, 12.803949356079102][32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=65521)[0m [CLIENT] Duration MAE = 0.0326, Destination Accuracy = 12.8039[32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=65518)[0m Full evaluation results: [278.0940856933594, 449.4546203613281, 5.5669121742248535, 0.0325581394135952, 12.256372451782227][32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=65518)[0m [CLIENT] Duration MAE = 0.0326, Destination Accuracy = 12.2564[32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=65520)[0m Full evaluation results: [260.4585266113281, 419.9252624511719, 5.549417972564697, 0.0325581394135952, 11.182330131530762][32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=65520)[0m [CLIENT] Duration MAE = 0.0326, Destination Accuracy = 11.1823[32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=65521)[0m Full evaluation results: [246.7294921875, 397.3964538574219, 5.565123081207275, 0.044186048209667206, 10.462335586547852][32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=65521)[0m [CLIENT] Duration MAE = 0.0442, Destination Accuracy = 10.4623[32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=65521)[0m Full evaluation results: [245.22792053222656, 394.7400207519531, 5.589733123779297, 0.03953488543629646, 10.350855827331543][32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=65521)[0m [CLIENT] Duration MAE = 0.0395, Destination Accuracy = 10.3509[32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=65521)[0m Full evaluation results: [237.72247314453125, 382.1976013183594, 5.6306891441345215, 0.03953488543629646, 10.018439292907715][32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=65521)[0m [CLIENT] Duration MAE = 0.0395, Destination Accuracy = 10.0184[32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=65521)[0m Full evaluation results: [225.51318359375, 361.8791809082031, 5.737828254699707, 0.01860465109348297, 9.437956809997559][32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=65521)[0m [CLIENT] Duration MAE = 0.0186, Destination Accuracy = 9.4380[32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=65521)[0m Full evaluation results: [216.93067932128906, 347.8388977050781, 5.7442851066589355, 0.05116279050707817, 9.139351844787598][32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=65521)[0m [CLIENT] Duration MAE = 0.0512, Destination Accuracy = 9.1394[32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=65521)[0m Full evaluation results: [204.60409545898438, 327.7676086425781, 5.821920871734619, 0.04883721098303795, 8.832243919372559][32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=65521)[0m [CLIENT] Duration MAE = 0.0488, Destination Accuracy = 8.8322[32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=65521)[0m Full evaluation results: [203.04837036132812, 325.4395446777344, 5.816685676574707, 0.04651162773370743, 8.845274925231934][32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=65521)[0m [CLIENT] Duration MAE = 0.0465, Destination Accuracy = 8.8453[32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=65521)[0m Full evaluation results: [200.0489501953125, 320.2656555175781, 5.8825225830078125, 0.04883721098303795, 8.962682723999023][32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=65521)[0m [CLIENT] Duration MAE = 0.0488, Destination Accuracy = 8.9627[32m [repeated 3x across cluster][0m
[36m(ClientAppActor pid=65518)[0m Full evaluation results: [200.0489501953125, 320.2656555175781, 5.8825225830078125, 0.04883721098303795, 8.962682723999023][32m [repeated 2x across cluster][0m
[36m(ClientAppActor pid=65518)[0m [CLIENT] Duration MAE = 0.0488, Destination Accuracy = 8.9627[32m [repeated 2x across cluster][0m

2025-04-08 17:49:37.305056: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-04-08 17:49:37.329548: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1744130977.345068  135014 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1744130977.349179  135014 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1744130977.368199  135014 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744130977.368252  135014 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744130977.368255  135014 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1744130977.368257  135014 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-04-08 17:49:37.372826: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[93mWARNING [0m:   DEPRECATED FEATURE: flwr.simulation.start_simulation() is deprecated.
	Instead, use the `flwr run` CLI command to start a local simulation in your Flower app, as shown for example below:

		$ flwr new  # Create a new Flower app from a template

		$ flwr run  # Run the Flower app in Simulation Mode

	Using `start_simulation()` is deprecated.

            This is a deprecated feature. It will be removed
            entirely in future versions of Flower.
        
[92mINFO [0m:      Starting Flower simulation, config: num_rounds=20, no round_timeout
2025-04-08 17:49:42,212	INFO worker.py:1771 -- Started a local Ray instance.
[92mINFO [0m:      Flower VCE: Ray initialized with resources: {'accelerator_type:G': 1.0, 'node:__internal_head__': 1.0, 'CPU': 8.0, 'object_store_memory': 4882524979.0, 'memory': 9765049959.0, 'GPU': 1.0, 'node:192.168.52.78': 1.0}
[92mINFO [0m:      Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html
[92mINFO [0m:      Flower VCE: Resources for each Virtual Client: {'num_cpus': 4}
[92mINFO [0m:      Flower VCE: Creating VirtualClientEngineActorPool with 2 actors
[92mINFO [0m:      [INIT]
[92mINFO [0m:      Requesting initial parameters from one random client
[36m(pid=135710)[0m 2025-04-08 17:49:43.210770: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[36m(pid=135710)[0m 2025-04-08 17:49:43.222243: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
[36m(pid=135710)[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
[36m(pid=135710)[0m E0000 00:00:1744130983.236160  135710 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
[36m(pid=135710)[0m E0000 00:00:1744130983.239908  135710 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
[36m(pid=135710)[0m W0000 00:00:1744130983.250717  135710 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[36m(pid=135710)[0m W0000 00:00:1744130983.250748  135710 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[36m(pid=135710)[0m W0000 00:00:1744130983.250750  135710 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[36m(pid=135710)[0m W0000 00:00:1744130983.250751  135710 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
[36m(pid=135710)[0m 2025-04-08 17:49:43.253951: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[36m(pid=135710)[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[36m(ClientAppActor pid=135710)[0m I0000 00:00:1744130985.578619  135710 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9
[92mINFO [0m:      Received initial parameters from one random client
[92mINFO [0m:      Starting evaluation of initial global parameters
[92mINFO [0m:      Evaluation returned no results (`None`)
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 1]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[36m(ClientAppActor pid=135710)[0m I0000 00:00:1744130988.524325  135903 service.cc:152] XLA service 0x7f27dc003fa0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
[36m(ClientAppActor pid=135710)[0m I0000 00:00:1744130988.524372  135903 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9
[36m(pid=135712)[0m 2025-04-08 17:49:43.210753: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
[36m(pid=135712)[0m 2025-04-08 17:49:43.222369: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
[36m(ClientAppActor pid=135710)[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(pid=135712)[0m E0000 00:00:1744130983.235871  135712 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
[36m(pid=135712)[0m E0000 00:00:1744130983.239878  135712 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
[36m(pid=135712)[0m W0000 00:00:1744130983.250751  135712 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.[32m [repeated 4x across cluster][0m
[36m(pid=135712)[0m 2025-04-08 17:49:43.253951: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
[36m(pid=135712)[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[36m(ClientAppActor pid=135710)[0m 2025-04-08 17:49:48.610958: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
[36m(ClientAppActor pid=135710)[0m I0000 00:00:1744130989.054034  135903 cuda_dnn.cc:529] Loaded cuDNN version 90300
[36m(ClientAppActor pid=135710)[0m 2025-04-08 17:49:49.949283: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 12 bytes spill stores, 12 bytes spill loads
[36m(ClientAppActor pid=135710)[0m 
[36m(ClientAppActor pid=135710)[0m 2025-04-08 17:49:50.239364: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_7', 40 bytes spill stores, 40 bytes spill loads
[36m(ClientAppActor pid=135710)[0m 
[36m(ClientAppActor pid=135710)[0m 2025-04-08 17:49:50.420581: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_7', 8 bytes spill stores, 8 bytes spill loads
[36m(ClientAppActor pid=135710)[0m 
[36m(ClientAppActor pid=135710)[0m 2025-04-08 17:49:50.571099: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_7', 40 bytes spill stores, 40 bytes spill loads
[36m(ClientAppActor pid=135710)[0m 
[36m(ClientAppActor pid=135712)[0m I0000 00:00:1744130986.438455  135712 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9
[36m(ClientAppActor pid=135712)[0m 
[36m(ClientAppActor pid=135712)[0m 
[36m(ClientAppActor pid=135712)[0m 
[36m(ClientAppActor pid=135712)[0m 
[36m(ClientAppActor pid=135710)[0m I0000 00:00:1744130992.543802  135903 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[36m(ClientAppActor pid=135712)[0m 
[36m(ClientAppActor pid=135712)[0m I0000 00:00:1744130989.207039  136006 service.cc:152] XLA service 0x7f93b8014770 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
[36m(ClientAppActor pid=135712)[0m I0000 00:00:1744130989.207117  136006 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9
[36m(ClientAppActor pid=135712)[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
[36m(ClientAppActor pid=135712)[0m 2025-04-08 17:49:49.258240: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
[36m(ClientAppActor pid=135712)[0m I0000 00:00:1744130989.672629  136006 cuda_dnn.cc:529] Loaded cuDNN version 90300
[36m(ClientAppActor pid=135712)[0m 
[36m(ClientAppActor pid=135712)[0m 
[36m(ClientAppActor pid=135712)[0m 
[36m(ClientAppActor pid=135710)[0m 
[36m(ClientAppActor pid=135710)[0m 2025-04-08 17:50:02.547959: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 12 bytes spill stores, 12 bytes spill loads[32m [repeated 9x across cluster][0m
[36m(ClientAppActor pid=135712)[0m I0000 00:00:1744130992.920237  136006 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[36m(ClientAppActor pid=135710)[0m 
[36m(ClientAppActor pid=135710)[0m 
[36m(ClientAppActor pid=135710)[0m 
[36m(ClientAppActor pid=135710)[0m 
[36m(ClientAppActor pid=135710)[0m 2025-04-08 17:50:10.420579: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 12 bytes spill stores, 12 bytes spill loads[32m [repeated 4x across cluster][0m
[36m(ClientAppActor pid=135710)[0m 
[36m(ClientAppActor pid=135710)[0m 
[36m(ClientAppActor pid=135710)[0m 
[36m(ClientAppActor pid=135712)[0m 
[36m(ClientAppActor pid=135712)[0m 2025-04-08 17:50:23.292953: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 12 bytes spill stores, 12 bytes spill loads[32m [repeated 4x across cluster][0m
[36m(ClientAppActor pid=135712)[0m 
[36m(ClientAppActor pid=135712)[0m 
[36m(ClientAppActor pid=135712)[0m 
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[93mWARNING [0m:   No fit_metrics_aggregation_fn provided
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[36m(ClientAppActor pid=135712)[0m 
[36m(ClientAppActor pid=135712)[0m 2025-04-08 17:50:28.319699: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2', 12 bytes spill stores, 12 bytes spill loads[32m [repeated 4x across cluster][0m
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[93mWARNING [0m:   No evaluate_metrics_aggregation_fn provided
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 2]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[36m(ClientAppActor pid=135712)[0m 
[36m(ClientAppActor pid=135712)[0m 2025-04-08 17:50:57.050550: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 12 bytes spill stores, 12 bytes spill loads
[36m(ClientAppActor pid=135712)[0m 2025-04-08 17:50:57.138950: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_7', 48 bytes spill stores, 48 bytes spill loads
[36m(ClientAppActor pid=135712)[0m 
[36m(ClientAppActor pid=135712)[0m 2025-04-08 17:50:57.170562: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_7', 32 bytes spill stores, 32 bytes spill loads
[36m(ClientAppActor pid=135712)[0m 
[36m(ClientAppActor pid=135712)[0m 2025-04-08 17:50:57.551255: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_7', 8 bytes spill stores, 8 bytes spill loads
[36m(ClientAppActor pid=135712)[0m 
[36m(ClientAppActor pid=135712)[0m 2025-04-08 17:51:05.956956: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_7', 32 bytes spill stores, 32 bytes spill loads
[36m(ClientAppActor pid=135712)[0m 
[36m(ClientAppActor pid=135712)[0m 2025-04-08 17:51:06.123808: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_7', 40 bytes spill stores, 40 bytes spill loads
[36m(ClientAppActor pid=135712)[0m 
[36m(ClientAppActor pid=135712)[0m 2025-04-08 17:51:06.125845: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_7', 8 bytes spill stores, 8 bytes spill loads
[36m(ClientAppActor pid=135712)[0m 
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[36m(ClientAppActor pid=135710)[0m 
[36m(ClientAppActor pid=135710)[0m 2025-04-08 17:51:15.141586: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2', 12 bytes spill stores, 12 bytes spill loads
[36m(ClientAppActor pid=135712)[0m 2025-04-08 17:51:19.699841: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2', 12 bytes spill stores, 12 bytes spill loads
[36m(ClientAppActor pid=135712)[0m 
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 3]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[36m(ClientAppActor pid=135710)[0m 
[36m(ClientAppActor pid=135710)[0m 2025-04-08 17:51:31.725910: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 12 bytes spill stores, 12 bytes spill loads
[36m(ClientAppActor pid=135710)[0m 2025-04-08 17:51:31.898017: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_7', 8 bytes spill stores, 8 bytes spill loads
[36m(ClientAppActor pid=135710)[0m 
[36m(ClientAppActor pid=135710)[0m 2025-04-08 17:51:32.278081: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_7', 40 bytes spill stores, 40 bytes spill loads
[36m(ClientAppActor pid=135710)[0m 
[36m(ClientAppActor pid=135710)[0m 2025-04-08 17:51:32.342240: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_7', 32 bytes spill stores, 32 bytes spill loads
[36m(ClientAppActor pid=135710)[0m 
[36m(ClientAppActor pid=135710)[0m 2025-04-08 17:51:54.518029: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_6', 12 bytes spill stores, 12 bytes spill loads
[36m(ClientAppActor pid=135710)[0m 
[36m(ClientAppActor pid=135710)[0m 2025-04-08 17:51:54.822414: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_7', 32 bytes spill stores, 32 bytes spill loads
[36m(ClientAppActor pid=135710)[0m 
[36m(ClientAppActor pid=135710)[0m 2025-04-08 17:51:55.098394: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_7', 48 bytes spill stores, 48 bytes spill loads
[36m(ClientAppActor pid=135710)[0m 
[36m(ClientAppActor pid=135710)[0m 2025-04-08 17:51:55.128513: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_7', 8 bytes spill stores, 8 bytes spill loads
[36m(ClientAppActor pid=135710)[0m 
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 4]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 5]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 6]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 7]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 8]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 9]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_fit: received 10 results and 0 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[92mINFO [0m:      aggregate_evaluate: received 10 results and 0 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 10]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 11]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 12]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 13]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 14]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 15]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[33m(raylet)[0m [2025-04-08 17:56:42,137 E 135187 135187] (raylet) node_manager.cc:3064: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836, IP: 192.168.52.78) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 192.168.52.78`
[33m(raylet)[0m 
[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 16]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 17]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 18]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 19]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [ROUND 20]
[92mINFO [0m:      configure_fit: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_fit: received 1 results and 9 failures
[92mINFO [0m:      configure_evaluate: strategy sampled 10 clients (out of 10)
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[91mERROR [0m:     Traceback (most recent call last):
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_client_proxy.py", line 95, in _submit_job
    out_mssg, updated_context = self.actor_pool.get_client_result(
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 401, in get_client_result
    return self._fetch_future_result(cid)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 282, in _fetch_future_result
    res_cid, out_mssg, updated_context = ray.get(
                                         ^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 2639, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/timotei/venv/lib/python3.12/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[91mERROR [0m:     Task was killed due to the node running low on memory.
Memory on the node (IP: 192.168.52.78, ID: 9277fb8769f2ed7b66973bf7ea4460d996e06d87e27fb5012ae0e836) where the task (actor ID: 33b73342aeb1bf27db2920aa01000000, name=ClientAppActor.__init__, pid=135712, memory used=6.31GB) was running was 18.59GB / 19.54GB (0.951355), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 192.168.52.78`. To see the logs of the worker, use `ray logs worker-d4afa5e690e4eea3f0e02840fb4a4ccc370d0f3b93308b642ba6d9ee*out -ip 192.168.52.78. Top 10 memory users:
PID	MEM(GB)	COMMAND
135712	6.31	ray::ClientAppActor
135710	6.30	ray::ClientAppActor
778	0.57	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node /home/timotei/.vscode...
539	0.49	/home/timotei/.vscode-server/bin/4437686ffebaf200fa4a6e6e67f735f3edf24ada/node --dns-result-order=ip...
135014	0.47	python3 fl_model/simulate_flower.py
134046	0.39	python3 fl_model/simulate_flower.py
135065	0.27	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
134111	0.26	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/gcs/gcs_server --log_dir=/tmp/ray/s...
135187	0.23	/home/timotei/venv/lib/python3.12/site-packages/ray/core/src/ray/raylet/raylet --raylet_socket_name=...
134769	0.19	ray::IDLE
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[92mINFO [0m:      aggregate_evaluate: received 1 results and 9 failures
[92mINFO [0m:      
[92mINFO [0m:      [SUMMARY]
[92mINFO [0m:      Run finished 20 round(s) in 449.02s
[92mINFO [0m:      	History (loss, distributed):
[92mINFO [0m:      		round 1: 1371.4146061408348
[92mINFO [0m:      		round 2: 1427.3415556793766
[92mINFO [0m:      		round 3: 1443.8456588513423
[92mINFO [0m:      		round 4: 1371.3249210522192
[92mINFO [0m:      		round 5: 1386.846790126743
[92mINFO [0m:      		round 6: 1454.056399488639
[92mINFO [0m:      		round 7: 1454.5720209069373
[92mINFO [0m:      		round 8: 1458.614599758047
[92mINFO [0m:      		round 9: 1430.4635113506577
[92mINFO [0m:      		round 10: 1707.364501953125
[92mINFO [0m:      		round 11: 1792.4556884765625
[92mINFO [0m:      		round 12: 2251.08447265625
[92mINFO [0m:      		round 13: 2337.319091796875
[92mINFO [0m:      		round 14: 1333.6197509765625
[92mINFO [0m:      		round 15: 1725.1585693359375
[92mINFO [0m:      		round 16: 1454.8057861328125
[92mINFO [0m:      		round 17: 1783.503173828125
[92mINFO [0m:      		round 18: 1373.2061767578125
[92mINFO [0m:      		round 19: 1599.6767578125
[92mINFO [0m:      		round 20: 1280.6783447265625
[92mINFO [0m:      
Memory growth enabled on GPUs
[36m(ClientAppActor pid=135710)[0m Memory growth enabled on GPUs
[36m(ClientAppActor pid=135712)[0m Full evaluation results: [1243.025146484375, 290.5101623535156, 2182.01123046875, 13.655356407165527, 2192.818115234375]
[36m(ClientAppActor pid=135712)[0m [CLIENT] Duration MAE = 13.6554, Length MAE = 2192.8181
[36m(ClientAppActor pid=135712)[0m Memory growth enabled on GPUs
[36m(ClientAppActor pid=135712)[0m Full evaluation results: [1376.2724609375, 279.956298828125, 2459.80126953125, 14.152030944824219, 2456.25537109375][32m [repeated 4x across cluster][0m
[36m(ClientAppActor pid=135712)[0m [CLIENT] Duration MAE = 14.1520, Length MAE = 2456.2554[32m [repeated 4x across cluster][0m
[36m(ClientAppActor pid=135712)[0m Full evaluation results: [1380.47509765625, 454.6942443847656, 2296.4755859375, 15.9473295211792, 2317.078369140625][32m [repeated 6x across cluster][0m
[36m(ClientAppActor pid=135712)[0m [CLIENT] Duration MAE = 15.9473, Length MAE = 2317.0784[32m [repeated 6x across cluster][0m
[36m(ClientAppActor pid=135710)[0m Full evaluation results: [1404.5599365234375, 338.3247375488281, 2456.45751953125, 16.11464500427246, 2452.915771484375][32m [repeated 8x across cluster][0m
[36m(ClientAppActor pid=135710)[0m [CLIENT] Duration MAE = 16.1146, Length MAE = 2452.9158[32m [repeated 8x across cluster][0m
[36m(ClientAppActor pid=135712)[0m Full evaluation results: [1165.46533203125, 375.2733459472656, 1955.34521484375, 16.020673751831055, 1955.8516845703125][32m [repeated 2x across cluster][0m
[36m(ClientAppActor pid=135712)[0m [CLIENT] Duration MAE = 16.0207, Length MAE = 1955.8517[32m [repeated 2x across cluster][0m
[36m(ClientAppActor pid=135712)[0m Full evaluation results: [1484.430419921875, 448.8074035644531, 2406.1328125, 16.97738265991211, 2492.613525390625][32m [repeated 8x across cluster][0m
[36m(ClientAppActor pid=135712)[0m [CLIENT] Duration MAE = 16.9774, Length MAE = 2492.6135[32m [repeated 8x across cluster][0m
[36m(ClientAppActor pid=135710)[0m Full evaluation results: [1484.218505859375, 421.4539489746094, 2416.970947265625, 16.188467025756836, 2501.400634765625][32m [repeated 2x across cluster][0m
[36m(ClientAppActor pid=135710)[0m [CLIENT] Duration MAE = 16.1885, Length MAE = 2501.4006[32m [repeated 2x across cluster][0m
[36m(ClientAppActor pid=135712)[0m Full evaluation results: [1405.131103515625, 529.4611206054688, 2294.908447265625, 16.80072784423828, 2250.1484375][32m [repeated 6x across cluster][0m
[36m(ClientAppActor pid=135712)[0m [CLIENT] Duration MAE = 16.8007, Length MAE = 2250.1484[32m [repeated 6x across cluster][0m
[36m(ClientAppActor pid=135712)[0m Full evaluation results: [1459.310791015625, 396.8632507324219, 2408.9716796875, 15.32925033569336, 2495.579345703125][32m [repeated 4x across cluster][0m
[36m(ClientAppActor pid=135712)[0m [CLIENT] Duration MAE = 15.3293, Length MAE = 2495.5793[32m [repeated 4x across cluster][0m
[36m(ClientAppActor pid=135712)[0m Full evaluation results: [1293.8714599609375, 307.6004638671875, 2167.96923828125, 13.692110061645508, 2241.767578125][32m [repeated 8x across cluster][0m
[36m(ClientAppActor pid=135712)[0m [CLIENT] Duration MAE = 13.6921, Length MAE = 2241.7676[32m [repeated 8x across cluster][0m
[36m(ClientAppActor pid=135712)[0m Full evaluation results: [1418.0205078125, 524.3578491210938, 2302.043701171875, 18.000980377197266, 2322.6982421875][32m [repeated 2x across cluster][0m
[36m(ClientAppActor pid=135712)[0m [CLIENT] Duration MAE = 18.0010, Length MAE = 2322.6982[32m [repeated 2x across cluster][0m
[36m(ClientAppActor pid=135712)[0m Full evaluation results: [1186.9722900390625, 412.2749938964844, 1962.4189453125, 17.03082847595215, 1962.952392578125][32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=135712)[0m [CLIENT] Duration MAE = 17.0308, Length MAE = 1962.9524[32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=135710)[0m Full evaluation results: [1548.180908203125, 540.989501953125, 2421.776611328125, 19.476449966430664, 2505.89111328125][32m [repeated 8x across cluster][0m
[36m(ClientAppActor pid=135710)[0m [CLIENT] Duration MAE = 19.4764, Length MAE = 2505.8911[32m [repeated 8x across cluster][0m
[36m(ClientAppActor pid=135712)[0m Full evaluation results: [1493.5987548828125, 456.5343322753906, 2623.314208984375, 17.430744171142578, 2560.926513671875][32m [repeated 2x across cluster][0m
[36m(ClientAppActor pid=135712)[0m [CLIENT] Duration MAE = 17.4307, Length MAE = 2560.9265[32m [repeated 2x across cluster][0m
[36m(ClientAppActor pid=135712)[0m Full evaluation results: [1307.4146728515625, 402.42340087890625, 2198.4453125, 17.21894073486328, 2209.25][32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=135712)[0m [CLIENT] Duration MAE = 17.2189, Length MAE = 2209.2500[32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=135710)[0m Full evaluation results: [1707.364501953125, 833.1094970703125, 2444.48486328125, 25.665233612060547, 2528.545166015625][32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=135710)[0m [CLIENT] Duration MAE = 25.6652, Length MAE = 2528.5452[32m [repeated 10x across cluster][0m
[36m(ClientAppActor pid=135710)[0m Full evaluation results: [1792.4556884765625, 974.6671142578125, 2462.673095703125, 28.669769287109375, 2546.877197265625]
[36m(ClientAppActor pid=135710)[0m [CLIENT] Duration MAE = 28.6698, Length MAE = 2546.8772
[36m(ClientAppActor pid=135710)[0m Full evaluation results: [2251.08447265625, 1928.9564208984375, 2525.5732421875, 43.26228332519531, 2522.25634765625]
[36m(ClientAppActor pid=135710)[0m [CLIENT] Duration MAE = 43.2623, Length MAE = 2522.2563
[36m(ClientAppActor pid=135710)[0m Full evaluation results: [2337.319091796875, 2366.059326171875, 2290.320556640625, 47.57430648803711, 2301.115478515625]
[36m(ClientAppActor pid=135710)[0m [CLIENT] Duration MAE = 47.5743, Length MAE = 2301.1155
[36m(ClientAppActor pid=135710)[0m Full evaluation results: [1333.6197509765625, 177.72653198242188, 2492.341796875, 11.871659278869629, 2489.133056640625]
[36m(ClientAppActor pid=135710)[0m [CLIENT] Duration MAE = 11.8717, Length MAE = 2489.1331
[36m(ClientAppActor pid=135710)[0m Full evaluation results: [1725.1585693359375, 1025.9439697265625, 2292.2265625, 30.263195037841797, 2365.422119140625]
[36m(ClientAppActor pid=135710)[0m [CLIENT] Duration MAE = 30.2632, Length MAE = 2365.4221
[36m(ClientAppActor pid=135710)[0m Full evaluation results: [1454.8057861328125, 224.27389526367188, 2685.06103515625, 10.651708602905273, 2683.330810546875]
[36m(ClientAppActor pid=135710)[0m [CLIENT] Duration MAE = 10.6517, Length MAE = 2683.3308
[36m(ClientAppActor pid=135710)[0m Full evaluation results: [1783.503173828125, 1021.70263671875, 2525.65869140625, 30.709522247314453, 2522.543212890625]
[36m(ClientAppActor pid=135710)[0m [CLIENT] Duration MAE = 30.7095, Length MAE = 2522.5432
[36m(ClientAppActor pid=135710)[0m Full evaluation results: [1373.2061767578125, 180.0307159423828, 2645.264892578125, 8.606185913085938, 2582.964111328125]
[36m(ClientAppActor pid=135710)[0m [CLIENT] Duration MAE = 8.6062, Length MAE = 2582.9641
[36m(ClientAppActor pid=135710)[0m Full evaluation results: [1599.6767578125, 686.316162109375, 2421.411865234375, 24.610992431640625, 2508.284912109375]
[36m(ClientAppActor pid=135710)[0m [CLIENT] Duration MAE = 24.6110, Length MAE = 2508.2849
[36m(ClientAppActor pid=135710)[0m Full evaluation results: [1280.6783447265625, 85.41867065429688, 2471.3828125, 7.136142253875732, 2468.297607421875]
[36m(ClientAppActor pid=135710)[0m [CLIENT] Duration MAE = 7.1361, Length MAE = 2468.2976
